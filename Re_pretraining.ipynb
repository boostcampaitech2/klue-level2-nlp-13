{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4690,
     "status": "ok",
     "timestamp": 1633478231696,
     "user": {
      "displayName": "â€ì„ê²½í˜„(ëŒ€í•™ì›í•™ìƒ/ì¼ë°˜ëŒ€í•™ì› ì»´í“¨í„°ê³¼í•™ê³¼)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "Fs-RUZNc46o8"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1633478242136,
     "user": {
      "displayName": "â€ì„ê²½í˜„(ëŒ€í•™ì›í•™ìƒ/ì¼ë°˜ëŒ€í•™ì› ì»´í“¨í„°ê³¼í•™ê³¼)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "oGtZLuF_4-N5"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...</td>\n",
       "      <td>{'word': 'ë¹„í‹€ì¦ˆ', 'start_idx': 24, 'end_idx': 26...</td>\n",
       "      <td>{'word': 'ì¡°ì§€ í•´ë¦¬ìŠ¨', 'start_idx': 13, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...</td>\n",
       "      <td>{'word': 'ë¯¼ì£¼í‰í™”ë‹¹', 'start_idx': 19, 'end_idx': ...</td>\n",
       "      <td>{'word': 'ëŒ€ì•ˆì‹ ë‹¹', 'start_idx': 14, 'end_idx': 1...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...</td>\n",
       "      <td>{'word': 'ê´‘ì£¼FC', 'start_idx': 21, 'end_idx': 2...</td>\n",
       "      <td>{'word': 'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹', 'start_idx': 34, 'end_idx...</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ê· ì¼ê°€ ìƒí™œìš©í’ˆì  (ì£¼)ì•„ì„±ë‹¤ì´ì†Œ(ëŒ€í‘œ ë°•ì •ë¶€)ëŠ” ì½”ë¡œë‚˜19 ë°”ì´ëŸ¬ìŠ¤ë¡œ ì–´ë ¤ì›€ì„ ê²ª...</td>\n",
       "      <td>{'word': 'ì•„ì„±ë‹¤ì´ì†Œ', 'start_idx': 13, 'end_idx': ...</td>\n",
       "      <td>{'word': 'ë°•ì •ë¶€', 'start_idx': 22, 'end_idx': 24...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967ë…„ í”„ë¡œ ì•¼êµ¬ ë“œë˜í”„íŠ¸ 1ìˆœìœ„ë¡œ ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ì—ê²Œ ì…ë‹¨í•˜ë©´ì„œ ë“±ë²ˆí˜¸ëŠ” 8...</td>\n",
       "      <td>{'word': 'ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ', 'start_idx': 22, 'end_id...</td>\n",
       "      <td>{'word': '1967', 'start_idx': 0, 'end_idx': 3,...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>7760</td>\n",
       "      <td>ì½”ë¡œë‚˜19 ë°©ì—­ ì¡°ì¹˜ì˜ ì¼í™˜ìœ¼ë¡œ êµ­ë¯¼ì˜ ì›€ì§ì„ì„ í†µì œí•˜ë ¤ëŠ” ì •ë¶€ì˜ ì‹œë„ë¥¼ ì´íƒˆë¦¬ì•„ ...</td>\n",
       "      <td>{'word': 'ì •ë¶€', 'start_idx': 33, 'end_idx': 34,...</td>\n",
       "      <td>{'word': 'ì´íƒˆë¦¬ì•„', 'start_idx': 41, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>7761</td>\n",
       "      <td>ì„  ì—°êµ¬ì›ì€ â€œìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì¹˜ë£Œì œì¸ ì¼€ì´ìº¡ì´ 92ì–µì› íŒë§¤ë˜ë©´ì„œ 2019ë…„ ì—°ê°„ 3...</td>\n",
       "      <td>{'word': 'ì¢…ê·¼ë‹¹', 'start_idx': 133, 'end_idx': 1...</td>\n",
       "      <td>{'word': 'ì „ë…„', 'start_idx': 143, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>7762</td>\n",
       "      <td>í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬(ì‚¬ì¥ ì¡°ì„±ì™„)ëŠ” 8ì›” 1ì¼ë¶€ë¡œ, 3ê¸‰ ê°„ë¶€ì§ì›ì— ëŒ€í•œ ìŠ¹ì§„Â·ì´ë™ ì¸...</td>\n",
       "      <td>{'word': 'í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬', 'start_idx': 0, 'end_idx'...</td>\n",
       "      <td>{'word': 'ì¡°ì„±ì™„', 'start_idx': 12, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>7763</td>\n",
       "      <td>1987ë…„ B. ìŠˆë‚˜ì´ë”(B. Schneider)ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œë‹¤.</td>\n",
       "      <td>{'word': 'B. ìŠˆë‚˜ì´ë”', 'start_idx': 6, 'end_idx':...</td>\n",
       "      <td>{'word': '1987ë…„', 'start_idx': 0, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>7764</td>\n",
       "      <td>ì´ìŠ¹ì˜¥ ê°•ì§„êµ°ìˆ˜ëŠ” 2ì¼ NHë†í˜‘ì€í–‰ ê°•ì§„êµ°ì§€ë¶€(ì§€ë¶€ì¥ ê°•ëŒ€í˜•)ë¥¼ ë°©ë¬¸í•´ ë¶€í’ˆÂ·ì†Œì¬Â·...</td>\n",
       "      <td>{'word': 'ì´ìŠ¹ì˜¥', 'start_idx': 0, 'end_idx': 2, ...</td>\n",
       "      <td>{'word': '2ì¼', 'start_idx': 10, 'end_idx': 11,...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40235 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence  \\\n",
       "0        0  ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...   \n",
       "1        1  í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...   \n",
       "2        2  Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...   \n",
       "3        3  ê· ì¼ê°€ ìƒí™œìš©í’ˆì  (ì£¼)ì•„ì„±ë‹¤ì´ì†Œ(ëŒ€í‘œ ë°•ì •ë¶€)ëŠ” ì½”ë¡œë‚˜19 ë°”ì´ëŸ¬ìŠ¤ë¡œ ì–´ë ¤ì›€ì„ ê²ª...   \n",
       "4        4  1967ë…„ í”„ë¡œ ì•¼êµ¬ ë“œë˜í”„íŠ¸ 1ìˆœìœ„ë¡œ ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ì—ê²Œ ì…ë‹¨í•˜ë©´ì„œ ë“±ë²ˆí˜¸ëŠ” 8...   \n",
       "...    ...                                                ...   \n",
       "7760  7760  ì½”ë¡œë‚˜19 ë°©ì—­ ì¡°ì¹˜ì˜ ì¼í™˜ìœ¼ë¡œ êµ­ë¯¼ì˜ ì›€ì§ì„ì„ í†µì œí•˜ë ¤ëŠ” ì •ë¶€ì˜ ì‹œë„ë¥¼ ì´íƒˆë¦¬ì•„ ...   \n",
       "7761  7761  ì„  ì—°êµ¬ì›ì€ â€œìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì¹˜ë£Œì œì¸ ì¼€ì´ìº¡ì´ 92ì–µì› íŒë§¤ë˜ë©´ì„œ 2019ë…„ ì—°ê°„ 3...   \n",
       "7762  7762  í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬(ì‚¬ì¥ ì¡°ì„±ì™„)ëŠ” 8ì›” 1ì¼ë¶€ë¡œ, 3ê¸‰ ê°„ë¶€ì§ì›ì— ëŒ€í•œ ìŠ¹ì§„Â·ì´ë™ ì¸...   \n",
       "7763  7763             1987ë…„ B. ìŠˆë‚˜ì´ë”(B. Schneider)ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œë‹¤.   \n",
       "7764  7764  ì´ìŠ¹ì˜¥ ê°•ì§„êµ°ìˆ˜ëŠ” 2ì¼ NHë†í˜‘ì€í–‰ ê°•ì§„êµ°ì§€ë¶€(ì§€ë¶€ì¥ ê°•ëŒ€í˜•)ë¥¼ ë°©ë¬¸í•´ ë¶€í’ˆÂ·ì†Œì¬Â·...   \n",
       "\n",
       "                                         subject_entity  \\\n",
       "0     {'word': 'ë¹„í‹€ì¦ˆ', 'start_idx': 24, 'end_idx': 26...   \n",
       "1     {'word': 'ë¯¼ì£¼í‰í™”ë‹¹', 'start_idx': 19, 'end_idx': ...   \n",
       "2     {'word': 'ê´‘ì£¼FC', 'start_idx': 21, 'end_idx': 2...   \n",
       "3     {'word': 'ì•„ì„±ë‹¤ì´ì†Œ', 'start_idx': 13, 'end_idx': ...   \n",
       "4     {'word': 'ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ', 'start_idx': 22, 'end_id...   \n",
       "...                                                 ...   \n",
       "7760  {'word': 'ì •ë¶€', 'start_idx': 33, 'end_idx': 34,...   \n",
       "7761  {'word': 'ì¢…ê·¼ë‹¹', 'start_idx': 133, 'end_idx': 1...   \n",
       "7762  {'word': 'í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬', 'start_idx': 0, 'end_idx'...   \n",
       "7763  {'word': 'B. ìŠˆë‚˜ì´ë”', 'start_idx': 6, 'end_idx':...   \n",
       "7764  {'word': 'ì´ìŠ¹ì˜¥', 'start_idx': 0, 'end_idx': 2, ...   \n",
       "\n",
       "                                          object_entity  \\\n",
       "0     {'word': 'ì¡°ì§€ í•´ë¦¬ìŠ¨', 'start_idx': 13, 'end_idx':...   \n",
       "1     {'word': 'ëŒ€ì•ˆì‹ ë‹¹', 'start_idx': 14, 'end_idx': 1...   \n",
       "2     {'word': 'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹', 'start_idx': 34, 'end_idx...   \n",
       "3     {'word': 'ë°•ì •ë¶€', 'start_idx': 22, 'end_idx': 24...   \n",
       "4     {'word': '1967', 'start_idx': 0, 'end_idx': 3,...   \n",
       "...                                                 ...   \n",
       "7760  {'word': 'ì´íƒˆë¦¬ì•„', 'start_idx': 41, 'end_idx': 4...   \n",
       "7761  {'word': 'ì „ë…„', 'start_idx': 143, 'end_idx': 14...   \n",
       "7762  {'word': 'ì¡°ì„±ì™„', 'start_idx': 12, 'end_idx': 14...   \n",
       "7763  {'word': '1987ë…„', 'start_idx': 0, 'end_idx': 4...   \n",
       "7764  {'word': '2ì¼', 'start_idx': 10, 'end_idx': 11,...   \n",
       "\n",
       "                          label     source  \n",
       "0                   no_relation  wikipedia  \n",
       "1                   no_relation   wikitree  \n",
       "2                 org:member_of   wikitree  \n",
       "3     org:top_members/employees   wikitree  \n",
       "4                   no_relation  wikipedia  \n",
       "...                         ...        ...  \n",
       "7760                        100   wikitree  \n",
       "7761                        100   wikitree  \n",
       "7762                        100   wikitree  \n",
       "7763                        100  wikipedia  \n",
       "7764                        100   wikitree  \n",
       "\n",
       "[40235 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../dataset/train/train.csv')\n",
    "test = pd.read_csv('../dataset/test/test_data.csv')\n",
    "\n",
    "merge = pd.concat([train,test])\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge['sentence'].to_csv('new.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "model = AutoModelForMaskedLM.from_pretrained('klue/roberta-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"new.csv\",\n",
    "    block_size=512,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21237,
     "status": "ok",
     "timestamp": 1633481385933,
     "user": {
      "displayName": "â€ì„ê²½í˜„(ëŒ€í•™ì›í•™ìƒ/ì¼ë°˜ëŒ€í•™ì› ì»´í“¨í„°ê³¼í•™ê³¼)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "GUjiJK6P4sbc",
    "outputId": "61c6f596-b2f6-44db-b9df-a6031992334f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at klue/roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Creating features from dataset file at /content/drive/MyDrive/Colab Notebooks/Relation_extraction/train.csv\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Db9cUEMc5aEq",
    "outputId": "752aaf7e-cbb1-4c4c-d28c-157f6241bd90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 32471\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 64944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13895' max='64944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13895/64944 1:38:46 < 6:02:56, 2.34 it/s, Epoch 1.71/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.827600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFNvX1N25xau"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMU4TXakQr5pSMi9r7f5Sa1",
   "collapsed_sections": [],
   "name": "Re_pretraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
