{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4690,
     "status": "ok",
     "timestamp": 1633478231696,
     "user": {
      "displayName": "‍임경현(대학원학생/일반대학원 컴퓨터과학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "Fs-RUZNc46o8"
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1633478242136,
     "user": {
      "displayName": "‍임경현(대학원학생/일반대학원 컴퓨터과학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "oGtZLuF_4-N5"
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>{'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...</td>\n",
       "      <td>{'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>{'word': '민주평화당', 'start_idx': 19, 'end_idx': ...</td>\n",
       "      <td>{'word': '대안신당', 'start_idx': 14, 'end_idx': 1...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>{'word': '광주FC', 'start_idx': 21, 'end_idx': 2...</td>\n",
       "      <td>{'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>{'word': '아성다이소', 'start_idx': 13, 'end_idx': ...</td>\n",
       "      <td>{'word': '박정부', 'start_idx': 22, 'end_idx': 24...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>{'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...</td>\n",
       "      <td>{'word': '1967', 'start_idx': 0, 'end_idx': 3,...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>7760</td>\n",
       "      <td>코로나19 방역 조치의 일환으로 국민의 움직임을 통제하려는 정부의 시도를 이탈리아 ...</td>\n",
       "      <td>{'word': '정부', 'start_idx': 33, 'end_idx': 34,...</td>\n",
       "      <td>{'word': '이탈리아', 'start_idx': 41, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>7761</td>\n",
       "      <td>선 연구원은 “위식도역류질환치료제인 케이캡이 92억원 판매되면서 2019년 연간 3...</td>\n",
       "      <td>{'word': '종근당', 'start_idx': 133, 'end_idx': 1...</td>\n",
       "      <td>{'word': '전년', 'start_idx': 143, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>7762</td>\n",
       "      <td>한국전기안전공사(사장 조성완)는 8월 1일부로, 3급 간부직원에 대한 승진·이동 인...</td>\n",
       "      <td>{'word': '한국전기안전공사', 'start_idx': 0, 'end_idx'...</td>\n",
       "      <td>{'word': '조성완', 'start_idx': 12, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>7763</td>\n",
       "      <td>1987년 B. 슈나이더(B. Schneider)에 의해 만들어졌다.</td>\n",
       "      <td>{'word': 'B. 슈나이더', 'start_idx': 6, 'end_idx':...</td>\n",
       "      <td>{'word': '1987년', 'start_idx': 0, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>7764</td>\n",
       "      <td>이승옥 강진군수는 2일 NH농협은행 강진군지부(지부장 강대형)를 방문해 부품·소재·...</td>\n",
       "      <td>{'word': '이승옥', 'start_idx': 0, 'end_idx': 2, ...</td>\n",
       "      <td>{'word': '2일', 'start_idx': 10, 'end_idx': 11,...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40235 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence  \\\n",
       "0        0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...   \n",
       "1        1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...   \n",
       "2        2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...   \n",
       "3        3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...   \n",
       "4        4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...   \n",
       "...    ...                                                ...   \n",
       "7760  7760  코로나19 방역 조치의 일환으로 국민의 움직임을 통제하려는 정부의 시도를 이탈리아 ...   \n",
       "7761  7761  선 연구원은 “위식도역류질환치료제인 케이캡이 92억원 판매되면서 2019년 연간 3...   \n",
       "7762  7762  한국전기안전공사(사장 조성완)는 8월 1일부로, 3급 간부직원에 대한 승진·이동 인...   \n",
       "7763  7763             1987년 B. 슈나이더(B. Schneider)에 의해 만들어졌다.   \n",
       "7764  7764  이승옥 강진군수는 2일 NH농협은행 강진군지부(지부장 강대형)를 방문해 부품·소재·...   \n",
       "\n",
       "                                         subject_entity  \\\n",
       "0     {'word': '비틀즈', 'start_idx': 24, 'end_idx': 26...   \n",
       "1     {'word': '민주평화당', 'start_idx': 19, 'end_idx': ...   \n",
       "2     {'word': '광주FC', 'start_idx': 21, 'end_idx': 2...   \n",
       "3     {'word': '아성다이소', 'start_idx': 13, 'end_idx': ...   \n",
       "4     {'word': '요미우리 자이언츠', 'start_idx': 22, 'end_id...   \n",
       "...                                                 ...   \n",
       "7760  {'word': '정부', 'start_idx': 33, 'end_idx': 34,...   \n",
       "7761  {'word': '종근당', 'start_idx': 133, 'end_idx': 1...   \n",
       "7762  {'word': '한국전기안전공사', 'start_idx': 0, 'end_idx'...   \n",
       "7763  {'word': 'B. 슈나이더', 'start_idx': 6, 'end_idx':...   \n",
       "7764  {'word': '이승옥', 'start_idx': 0, 'end_idx': 2, ...   \n",
       "\n",
       "                                          object_entity  \\\n",
       "0     {'word': '조지 해리슨', 'start_idx': 13, 'end_idx':...   \n",
       "1     {'word': '대안신당', 'start_idx': 14, 'end_idx': 1...   \n",
       "2     {'word': '한국프로축구연맹', 'start_idx': 34, 'end_idx...   \n",
       "3     {'word': '박정부', 'start_idx': 22, 'end_idx': 24...   \n",
       "4     {'word': '1967', 'start_idx': 0, 'end_idx': 3,...   \n",
       "...                                                 ...   \n",
       "7760  {'word': '이탈리아', 'start_idx': 41, 'end_idx': 4...   \n",
       "7761  {'word': '전년', 'start_idx': 143, 'end_idx': 14...   \n",
       "7762  {'word': '조성완', 'start_idx': 12, 'end_idx': 14...   \n",
       "7763  {'word': '1987년', 'start_idx': 0, 'end_idx': 4...   \n",
       "7764  {'word': '2일', 'start_idx': 10, 'end_idx': 11,...   \n",
       "\n",
       "                          label     source  \n",
       "0                   no_relation  wikipedia  \n",
       "1                   no_relation   wikitree  \n",
       "2                 org:member_of   wikitree  \n",
       "3     org:top_members/employees   wikitree  \n",
       "4                   no_relation  wikipedia  \n",
       "...                         ...        ...  \n",
       "7760                        100   wikitree  \n",
       "7761                        100   wikitree  \n",
       "7762                        100   wikitree  \n",
       "7763                        100  wikipedia  \n",
       "7764                        100   wikitree  \n",
       "\n",
       "[40235 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../dataset/train/train.csv')\n",
    "test = pd.read_csv('../dataset/test/test_data.csv')\n",
    "\n",
    "merge = pd.concat([train,test])\n",
    "merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge['sentence'].to_csv('new.csv',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "model = AutoModelForMaskedLM.from_pretrained('klue/roberta-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"new.csv\",\n",
    "    block_size=512,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21237,
     "status": "ok",
     "timestamp": 1633481385933,
     "user": {
      "displayName": "‍임경현(대학원학생/일반대학원 컴퓨터과학과)",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64",
      "userId": "16529317396415739634"
     },
     "user_tz": -540
    },
    "id": "GUjiJK6P4sbc",
    "outputId": "61c6f596-b2f6-44db-b9df-a6031992334f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n",
      "loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertTokenizer\",\n",
      "  \"transformers_version\": \"4.11.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at klue/roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Creating features from dataset file at /content/drive/MyDrive/Colab Notebooks/Relation_extraction/train.csv\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Db9cUEMc5aEq",
    "outputId": "752aaf7e-cbb1-4c4c-d28c-157f6241bd90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 32471\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 64944\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13895' max='64944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13895/64944 1:38:46 < 6:02:56, 2.34 it/s, Epoch 1.71/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.910300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.817400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.855400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.827600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.843000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.815400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.798800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500\n",
      "Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/config.json\n",
      "Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFNvX1N25xau"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMU4TXakQr5pSMi9r7f5Sa1",
   "collapsed_sections": [],
   "name": "Re_pretraining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
