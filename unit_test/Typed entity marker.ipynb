{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de431c0d-8ec3-4763-8b56-f4bbb860a782",
   "metadata": {},
   "source": [
    "# Using special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b3e1c51-cc6c-4e44-925b-deb5f85672a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd5afaa-a1de-4bd5-9580-7ce059b6e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_config =  AutoConfig.from_pretrained('klue/bert-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', config=model_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "909b9ab7-d8f4-4e33-8f18-80493dc6de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "    subject_word, subject_idx, subject_type = [], [], []\n",
    "    object_word, object_idx, object_type = [], [], []\n",
    "\n",
    "    for subject_entity, object_entity in zip(dataset['subject_entity'], dataset['object_entity']):\n",
    "        subject_dict = eval(subject_entity)\n",
    "        object_dict = eval(object_entity)\n",
    "\n",
    "        subject_word.append(subject_dict['word'])\n",
    "        subject_idx.append((subject_dict['start_idx'], subject_dict['end_idx']))\n",
    "        subject_type.append(subject_dict['type'])\n",
    "        object_word.append(object_dict['word'])\n",
    "        object_idx.append((object_dict['start_idx'], object_dict['end_idx']))\n",
    "        object_type.append(object_dict['type'])\n",
    "\n",
    "    out_dataset = pd.DataFrame({\n",
    "        'id': dataset['id'], \n",
    "        'sentence': dataset['sentence'],\n",
    "        'subject_word': subject_word,\n",
    "        'subject_idx': subject_idx,\n",
    "        'subject_type': subject_type,\n",
    "        'object_word': object_word,\n",
    "        'object_idx': object_idx,\n",
    "        'object_type': object_type,\n",
    "        'label': dataset['label'],\n",
    "        'source': dataset['source']\n",
    "    })\n",
    "\n",
    "    return out_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8779a9cc-5df1-4772-a942-66ca4c37b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset = pd.read_csv('../../../dataset/train/train.csv')\n",
    "dataset = preprocessing_dataset(pd_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa1c3f2-71bd-41e0-a64b-6f7fe799f34f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_word</th>\n",
       "      <th>subject_idx</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object_word</th>\n",
       "      <th>object_idx</th>\n",
       "      <th>object_type</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>비틀즈</td>\n",
       "      <td>(24, 26)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>조지 해리슨</td>\n",
       "      <td>(13, 18)</td>\n",
       "      <td>PER</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>민주평화당</td>\n",
       "      <td>(19, 23)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>대안신당</td>\n",
       "      <td>(14, 17)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>광주FC</td>\n",
       "      <td>(21, 24)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>한국프로축구연맹</td>\n",
       "      <td>(34, 41)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>아성다이소</td>\n",
       "      <td>(13, 17)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>박정부</td>\n",
       "      <td>(22, 24)</td>\n",
       "      <td>PER</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>요미우리 자이언츠</td>\n",
       "      <td>(22, 30)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1967</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>DAT</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_word  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          비틀즈   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...        민주평화당   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...         광주FC   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...        아성다이소   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...    요미우리 자이언츠   \n",
       "\n",
       "  subject_idx subject_type object_word object_idx object_type  \\\n",
       "0    (24, 26)          ORG      조지 해리슨   (13, 18)         PER   \n",
       "1    (19, 23)          ORG        대안신당   (14, 17)         ORG   \n",
       "2    (21, 24)          ORG    한국프로축구연맹   (34, 41)         ORG   \n",
       "3    (13, 17)          ORG         박정부   (22, 24)         PER   \n",
       "4    (22, 30)          ORG        1967     (0, 3)         DAT   \n",
       "\n",
       "                       label     source  \n",
       "0                no_relation  wikipedia  \n",
       "1                no_relation   wikitree  \n",
       "2              org:member_of   wikitree  \n",
       "3  org:top_members/employees   wikitree  \n",
       "4                no_relation  wikipedia  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fbc69b4-57b2-4b55-997b-c4ac8f15dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pickle\n",
    "\n",
    "def label_to_num(file_path, label):\n",
    "    num_label = []\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dict_label_to_num = pickle.load(f)\n",
    "    for v in label:\n",
    "        num_label.append(dict_label_to_num[v])\n",
    "\n",
    "    return num_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6dc6da2-5b75-412f-92f8-d27541048576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 20, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset\n",
    "train_label = label_to_num('../dict_label_to_num.pkl', train_dataset['label'].values) \n",
    "train_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9858b36f-b9c0-404b-a2e2-f5f27ec54045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_word</th>\n",
       "      <th>subject_idx</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object_word</th>\n",
       "      <th>object_idx</th>\n",
       "      <th>object_type</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>비틀즈</td>\n",
       "      <td>(24, 26)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>조지 해리슨</td>\n",
       "      <td>(13, 18)</td>\n",
       "      <td>PER</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...</td>\n",
       "      <td>민주평화당</td>\n",
       "      <td>(19, 23)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>대안신당</td>\n",
       "      <td>(14, 17)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...</td>\n",
       "      <td>광주FC</td>\n",
       "      <td>(21, 24)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>한국프로축구연맹</td>\n",
       "      <td>(34, 41)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...</td>\n",
       "      <td>아성다이소</td>\n",
       "      <td>(13, 17)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>박정부</td>\n",
       "      <td>(22, 24)</td>\n",
       "      <td>PER</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...</td>\n",
       "      <td>요미우리 자이언츠</td>\n",
       "      <td>(22, 30)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>1967</td>\n",
       "      <td>(0, 3)</td>\n",
       "      <td>DAT</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_word  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          비틀즈   \n",
       "1   1  호남이 기반인 바른미래당·대안신당·민주평화당이 우여곡절 끝에 합당해 민생당(가칭)으...        민주평화당   \n",
       "2   2  K리그2에서 성적 1위를 달리고 있는 광주FC는 지난 26일 한국프로축구연맹으로부터...         광주FC   \n",
       "3   3  균일가 생활용품점 (주)아성다이소(대표 박정부)는 코로나19 바이러스로 어려움을 겪...        아성다이소   \n",
       "4   4  1967년 프로 야구 드래프트 1순위로 요미우리 자이언츠에게 입단하면서 등번호는 8...    요미우리 자이언츠   \n",
       "\n",
       "  subject_idx subject_type object_word object_idx object_type  \\\n",
       "0    (24, 26)          ORG      조지 해리슨   (13, 18)         PER   \n",
       "1    (19, 23)          ORG        대안신당   (14, 17)         ORG   \n",
       "2    (21, 24)          ORG    한국프로축구연맹   (34, 41)         ORG   \n",
       "3    (13, 17)          ORG         박정부   (22, 24)         PER   \n",
       "4    (22, 30)          ORG        1967     (0, 3)         DAT   \n",
       "\n",
       "                       label     source  \n",
       "0                no_relation  wikipedia  \n",
       "1                no_relation   wikitree  \n",
       "2              org:member_of   wikitree  \n",
       "3  org:top_members/employees   wikitree  \n",
       "4                no_relation  wikipedia  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86f7fe81-b619-416a-9373-e2c4fb991ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset(dataset, tokenizer):\n",
    "    \"\"\" tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "    concat_entity = []\n",
    "    for e01, e02 in zip(dataset['subject_word'], dataset['object_word']):\n",
    "        temp = ''\n",
    "        temp = e01 + '[SEP]' + e02\n",
    "        concat_entity.append(temp)\n",
    "    tokenized_sentences = tokenizer(\n",
    "          concat_entity,\n",
    "          list(dataset['sentence']),\n",
    "          return_tensors=\"pt\",\n",
    "          padding=True,\n",
    "          truncation=True,\n",
    "          max_length=256,\n",
    "          add_special_tokens=True,\n",
    "          #return_token_type_ids=False,\n",
    "    )\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa27bc93-1845-4c15-a16b-ec622847b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_dataset(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0db2ca74-c500-4ef1-858b-299783332f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '비틀즈', '[SEP]', '조지', '해리', '##슨', '[SEP]', '〈', 'So', '##me']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0].tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72b48e4d-0007-4533-8f08-b2f34e6d5092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_word</th>\n",
       "      <th>subject_idx</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object_word</th>\n",
       "      <th>object_idx</th>\n",
       "      <th>object_type</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...</td>\n",
       "      <td>비틀즈</td>\n",
       "      <td>(24, 26)</td>\n",
       "      <td>ORG</td>\n",
       "      <td>조지 해리슨</td>\n",
       "      <td>(13, 18)</td>\n",
       "      <td>PER</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence subject_word  \\\n",
       "0   0  〈Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey R...          비틀즈   \n",
       "\n",
       "  subject_idx subject_type object_word object_idx object_type        label  \\\n",
       "0    (24, 26)          ORG      조지 해리슨   (13, 18)         PER  no_relation   \n",
       "\n",
       "      source  \n",
       "0  wikipedia  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c6c423ff-1434-447f-b2e9-ab77b82d0e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〈Something〉는 [PER]조지 해리슨[/PER]이 쓰고 [ORG]비틀즈[/ORG]가 1969년 앨범 《Abbey Road》에 담은 노래다.\n"
     ]
    }
   ],
   "source": [
    "if dataset[\"subject_idx\"][0] > dataset[\"object_idx\"][0]:\n",
    "    print(dataset[\"sentence\"][0][:dataset[\"object_idx\"][0][0]]\n",
    "          + f'[{dataset[\"object_type\"][0]}]' + dataset['object_word'][0] + f'[/{dataset[\"object_type\"][0]}]'\n",
    "          + dataset[\"sentence\"][0][dataset[\"object_idx\"][0][1]+1:dataset[\"subject_idx\"][0][0]]\n",
    "          + f'[{dataset[\"subject_type\"][0]}]' + dataset['subject_word'][0] + f'[/{dataset[\"subject_type\"][0]}]'\n",
    "          + dataset[\"sentence\"][0][dataset[\"subject_idx\"][0][1]+1:]\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944b528-f7db-4c76-8f83-d500af0a3af4",
   "metadata": {},
   "source": [
    "## Typed entity marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "14486344-966a-4ff8-9a63-e8530bc02f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset_custom(dataset, tokenizer):\n",
    "    sentence_list = []\n",
    "    seperate_token = '[SEP]'\n",
    "    for s_word, s_type, s_idx, o_word, o_type, o_idx, sentence in zip(dataset['subject_word'], dataset['subject_type'], dataset['subject_idx'],\n",
    "                                                                        dataset['object_word'], dataset['object_type'], dataset['object_idx'], dataset['sentence']):\n",
    "        # s_idx = eval(s_idx)\n",
    "        # o_idx = eval(o_idx)\n",
    "        if s_idx > o_idx:\n",
    "            sentence_list.append(sentence[:o_idx[0]] + f'<O:{o_type}>' + o_word + f'</O:{o_type}>' + sentence[o_idx[1]+1:s_idx[0]] \\\n",
    "                                 + seperate_token + f'<S:{s_type}>' + s_word + f'</S:{s_type}>' + sentence[s_idx[1]+1:])\n",
    "        else:\n",
    "            sentence_list.append(sentence[:s_idx[0]] + f'<S:{s_type}>' + s_word + f'</S:{s_type}>' + sentence[s_idx[1]+1:o_idx[0]] \\\n",
    "                                 + seperate_token + f'<O:{o_type}>' + o_word + f'</O:{o_type}>' + sentence[o_idx[1]+1:])\n",
    "\n",
    "\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentence_list,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=True,\n",
    "        #return_token_type_ids=False, # 문장 id\n",
    "    )\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2c35b6c2-0ee3-4170-aa3b-546afdc5bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,   168, 30985,  ...,     0,     0,     0],\n",
       "        [    2,  6409,  2052,  ...,     0,     0,     0],\n",
       "        [    2,    47, 17665,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2,    32,    55,  ...,     0,     0,     0],\n",
       "        [    2, 15724,    16,  ...,     0,     0,     0],\n",
       "        [    2,    32,    55,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train = tokenized_dataset_custom(dataset, tokenizer)\n",
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2de9832e-017a-471d-830e-c78ba2cbaf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['<S:PER>','<S:ORG>','<S:POH>','<S:DAT>','<S:LOC>','<S:NOH>','<O:PER>','<O:ORG>','<O:POH>','<O:DAT>','<O:LOC>','<O:NOH>','</S:PER>','</S:ORG>','</S:POH>','</S:DAT>','</S:LOC>','</S:NOH>','</O:PER>','</O:ORG>','</O:POH>','</O:DAT>','</O:LOC>','</O:NOH>']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "632134d9-ebed-4e1e-92b2-638cfd438823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['<S:PER>', '<S:ORG>', '<S:POH>', '<S:DAT>', '<S:LOC>', '<S:NOH>', '<O:PER>', '<O:ORG>', '<O:POH>', '<O:DAT>', '<O:LOC>', '<O:NOH>', '</S:PER>', '</S:ORG>', '</S:POH>', '</S:DAT>', '</S:LOC>', '</S:NOH>', '</O:PER>', '</O:ORG>', '</O:POH>', '</O:DAT>', '</O:LOC>', '</O:NOH>']})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e315b016-b972-4569-b670-e4f75b9060b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 168, 30985, 14451, 7088, 4586, 169, 793, 8373, 14113, 2234, 2052, 1363, 2088, 29830, 2116, 14879, 2440, 6711, 170, 21406, 26713, 2076, 25145, 5749, 171, 1421, 818, 2073, 4388, 2062, 18, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 〈 Something 〉 는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《 Abbey Road 》 에 담은 노래다. [SEP]'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.encode(train_dataset.sentence.iloc[0]))\n",
    "#print(tokenizer.encode(train_dataset.sentence.iloc[0]).token_type_ids)\n",
    "tokenizer.decode(tokenizer.encode(train_dataset.sentence.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "22a9d7fa-db2d-4d3e-b6da-77c46fd424e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Dataset 구성을 위한 class.\"\"\"\n",
    "    def __init__(self, pair_dataset, labels):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24b81fb2-b32a-434f-b3e1-0e8498a8c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_train_dataset = RE_Dataset(tokenized_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55248e41-348f-407b-b908-e873507ac961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 768)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538cf49d-ebc4-43be-b485-1a25e1460699",
   "metadata": {},
   "source": [
    "## Typed entity marker Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5cb25f8b-f274-48ab-92c7-d127b8c43336",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2926d4e4-dc65-4742-a79c-293fa07afb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_dataset_custom(dataset, tokenizer):\n",
    "    sentence_list = []\n",
    "\n",
    "    for s_word, s_type, s_idx, o_word, o_type, o_idx, sentence in zip(dataset['subject_word'], dataset['subject_type'], dataset['subject_idx'],\n",
    "                                                                        dataset['object_word'], dataset['object_type'], dataset['object_idx'], dataset['sentence']):\n",
    "        # s_idx = eval(s_idx)\n",
    "        # o_idx = eval(o_idx)\n",
    "        if s_idx > o_idx:\n",
    "            sentence_list.append(sentence[:o_idx[0]] + f' # ∧ [{o_type}] ∧ {o_word} # ' + sentence[o_idx[1]+1:s_idx[0]] \\\n",
    "                                 + f' @ * {s_type} * {s_word} @ ' + sentence[s_idx[1]+1:])\n",
    "        else:\n",
    "            sentence_list.append(sentence[:s_idx[0]] + f' @ * {s_type} * {s_word} @ ' + sentence[s_idx[1]+1:o_idx[0]] \\\n",
    "                                 + f' # ∧ {o_type} ∧ {o_word} # ' + sentence[o_idx[1]+1:])\n",
    "\n",
    "    print(sentence_list[0])\n",
    "    tokenized_sentences = tokenizer(\n",
    "        sentence_list,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=True,\n",
    "        #return_token_type_ids=False, # 문장 id\n",
    "    )\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b576ac1d-9ee1-4b66-b42f-ef40a4167ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='klue/roberta-large', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[PER]', 'ORG', 'LOC', 'POH', 'NOH', 'DAT']})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[PER]', 'ORG', 'LOC', 'POH', 'NOH', 'DAT']}\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "174585d5-eba7-479f-8326-201ddad588c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "〈Something〉는  # ∧ [PER] ∧ 조지 해리슨 # 이 쓰고  @ * ORG * 비틀즈 @ 가 1969년 앨범 《Abbey Road》에 담은 노래다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 〈 Something 〉 는 # [UNK] [ PER ] [UNK] 조지 해리슨 # 이 쓰고 @ * ORG * 비틀즈 @ 가 1969년 앨범 《 Abbey Road 》 에 담은 노래다. [SEP]'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_dataset_custom(dataset.head(1), tokenizer).input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922d365-42ba-4187-b87f-673502c01693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
