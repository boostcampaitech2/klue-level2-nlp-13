{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Re_pretraining.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMU4TXakQr5pSMi9r7f5Sa1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EvHRm-bq4rby","executionInfo":{"status":"ok","timestamp":1633478192335,"user_tz":-540,"elapsed":23934,"user":{"displayName":"‍임경현(대학원학생/일반대학원 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64","userId":"16529317396415739634"}},"outputId":"b3b0876f-9159-457a-bc40-ea7a747c05cb"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"Fs-RUZNc46o8","executionInfo":{"status":"ok","timestamp":1633478231696,"user_tz":-540,"elapsed":4690,"user":{"displayName":"‍임경현(대학원학생/일반대학원 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64","userId":"16529317396415739634"}}},"source":["from transformers import RobertaTokenizer, RobertaForMaskedLM, AutoTokenizer, AutoModelForMaskedLM\n","from transformers import LineByLineTextDataset\n","from sklearn.model_selection import train_test_split\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import os"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"oGtZLuF_4-N5","executionInfo":{"status":"ok","timestamp":1633478242136,"user_tz":-540,"elapsed":1083,"user":{"displayName":"‍임경현(대학원학생/일반대학원 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64","userId":"16529317396415739634"}}},"source":["os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GUjiJK6P4sbc","executionInfo":{"status":"ok","timestamp":1633481385933,"user_tz":-540,"elapsed":21237,"user":{"displayName":"‍임경현(대학원학생/일반대학원 컴퓨터과학과)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhD9DCTi6sILUGGMHEMhZ7L4GmuoLIdSUtE4KOC=s64","userId":"16529317396415739634"}},"outputId":"61c6f596-b2f6-44db-b9df-a6031992334f"},"source":["tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n","model = AutoModelForMaskedLM.from_pretrained('klue/roberta-large')\n","\n","#train_dataset, valid_dataset = train_test_split(dataset, test_size=0.2, stratify=dataset['label'], shuffle=True, random_state=42)\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/train.csv\",\n","    block_size=512,\n",")   \n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")\n","\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=8,\n","    per_device_train_batch_size=4,\n","    save_steps=500,\n","    save_total_limit=5,\n","    seed=42,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,    \n",")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.11.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/4eb906e7d0da2b04e56c7cc31ba068d7c295240a51690153c2ced71c9e4c9fc5.d1b86bed49516351c7bb29b19d7e7be2ab53b931bcb1f9b2aacfb71f2124d25a\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/1a24ab4628028ed80dea35ce3334a636dc656fd9a17a09bad377f88f0cbecdac.70c17d6e4d492c8f24f5bb97ab56c7f272e947112c6faf9dd846da42ba13eb23\n","loading file https://huggingface.co/klue/roberta-large/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8f31ccbd66730704a8400c96db0647b10c47cd0c838ea2cabf0a86ef878f31cf.1e08907f1658efd26357385b59d5a6567fc8faf443082618493ad3a2a1a45f0f\n","loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.11.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.11.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading configuration file https://huggingface.co/klue/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/571e05a2160c18c93365862223c4dae92bbd1b41464a4bd5f372ad703dba6097.ae5b7f8d8a28a3ff0b1560b4d08c6c3bd80f627288eee2024e02959dd60380d0\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"tokenizer_class\": \"BertTokenizer\",\n","  \"transformers_version\": \"4.11.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file https://huggingface.co/klue/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fd91c85effc137c99cd14cfe5c3459faa223c005b1577dc2c5aa48f6b2c4fbb1.3d5d467e78cd19d9a87029910ed83289edde0111a75a41e0cc79ad3fc06e4a51\n","All model checkpoint weights were used when initializing RobertaForMaskedLM.\n","\n","All the weights of RobertaForMaskedLM were initialized from the model checkpoint at klue/roberta-large.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n","/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n","  FutureWarning,\n","Creating features from dataset file at /content/drive/MyDrive/Colab Notebooks/Relation_extraction/train.csv\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Db9cUEMc5aEq","outputId":"752aaf7e-cbb1-4c4c-d28c-157f6241bd90"},"source":["trainer.train()\n","trainer.save_model(\"/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained_model\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 32471\n","  Num Epochs = 8\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 64944\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13895' max='64944' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13895/64944 1:38:46 < 6:02:56, 2.34 it/s, Epoch 1.71/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.910300</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.817400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.850000</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.834300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.855400</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.850600</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>0.855900</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>0.825600</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>0.850600</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>0.847500</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>0.829700</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>0.831400</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>0.827600</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>0.819100</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>0.843000</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>0.815400</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>0.805200</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>0.803000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>0.795000</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>0.782100</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>0.798800</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>0.799100</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>0.785400</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>0.776000</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>0.801900</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>0.776500</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>0.768900</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-2500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-7500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-8500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-9500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-12500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10000] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13000/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-10500] due to args.save_total_limit\n","Saving model checkpoint to /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500\n","Configuration saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/config.json\n","Model weights saved in /content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-13500/pytorch_model.bin\n","Deleting older checkpoint [/content/drive/MyDrive/Colab Notebooks/Relation_extraction/roberta-retrained/checkpoint-11000] due to args.save_total_limit\n"]}]},{"cell_type":"code","metadata":{"id":"eFNvX1N25xau"},"source":[""],"execution_count":null,"outputs":[]}]}