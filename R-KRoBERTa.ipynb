{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle as pickle\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from transformers import AutoConfig, RobertaModel, AutoModel\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import random\r\n",
    "from itertools import chain\r\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\r\n",
    "import copy\r\n",
    "import csv\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import os\r\n",
    "import torch.nn as nn\r\n",
    "from tqdm.auto import tqdm\r\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\r\n",
    "import torch.nn.functional as F\r\n",
    "\r\n",
    "from transformers import AutoTokenizer\r\n",
    "#from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\r\n",
    "# from transformers import ElectraModel, ElectraConfig, ElectraPreTrainedModel, ElectraTokenizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "#import wandb\r\n",
    "import yaml\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "import sklearn\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "random_seed=42\r\n",
    "torch.manual_seed(random_seed)\r\n",
    "torch.cuda.manual_seed(random_seed)\r\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\r\n",
    "torch.backends.cudnn.deterministic = True\r\n",
    "torch.backends.cudnn.benchmark = False\r\n",
    "np.random.seed(random_seed)\r\n",
    "random.seed(random_seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dataset 구성.\r\n",
    "class RE_Dataset(torch.utils.data.Dataset):\r\n",
    "  \"\"\" Dataset 구성을 위한 class.\"\"\"\r\n",
    "  def __init__(self, pair_dataset, label):\r\n",
    "    self.pair_dataset = pair_dataset\r\n",
    "    self.label = label\r\n",
    "\r\n",
    "  def __getitem__(self, idx):\r\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\r\n",
    "    item['label'] = torch.tensor(self.label[idx])\r\n",
    "    return item\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "    return len(self.label)\r\n",
    "\r\n",
    "def preprocessing_dataset(dataset):\r\n",
    "  \"\"\" 처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\r\n",
    "  subject_entity = []\r\n",
    "  object_entity = []\r\n",
    "  sub_st = []\r\n",
    "  sub_end = []\r\n",
    "  obj_st = []\r\n",
    "  obj_end = []\r\n",
    "  for i,j in zip(dataset['subject_entity'], dataset['object_entity']):\r\n",
    "    sub_dic = yaml.load(i)\r\n",
    "    obj_dic = yaml.load(j)\r\n",
    "\r\n",
    "    sub = sub_dic['word']\r\n",
    "    obj = obj_dic['word']\r\n",
    "    e1_st_idx = int(sub_dic['start_idx'])\r\n",
    "    e1_end_idx = int(sub_dic['end_idx'])\r\n",
    "    e2_st_idx = int(obj_dic['start_idx'])\r\n",
    "    e2_end_idx =  int(obj_dic['end_idx'])\r\n",
    "\r\n",
    "    subject_entity.append(sub)\r\n",
    "    object_entity.append(obj)\r\n",
    "    sub_st.append(e1_st_idx)\r\n",
    "    sub_end.append(e1_end_idx)\r\n",
    "    obj_st.append(e2_st_idx)\r\n",
    "    obj_end.append(e2_end_idx)\r\n",
    "\r\n",
    "  out_dataset = pd.DataFrame({'id':dataset['id'], 'sentence':dataset['sentence'],'subject_entity':subject_entity,\\\r\n",
    "                              'object_entity':object_entity,'sub_st' : sub_st,'sub_end': sub_end, 'obj_st': obj_st ,\\\r\n",
    "                              'obj_end': obj_end,'label':dataset['label']})\r\n",
    "  return out_dataset\r\n",
    "\r\n",
    "def load_data(dataset_dir):\r\n",
    "  \"\"\" csv 파일을 경로에 맡게 불러 옵니다. \"\"\"\r\n",
    "  pd_dataset = pd.read_csv(dataset_dir)\r\n",
    "  dataset = preprocessing_dataset(pd_dataset)\r\n",
    "  \r\n",
    "  return dataset\r\n",
    "\r\n",
    "def label_to_num(label):\r\n",
    "  num_label = []\r\n",
    "  with open('/opt/ml/code/dict_label_to_num.pkl', 'rb') as f:\r\n",
    "    dict_label_to_num = pickle.load(f)\r\n",
    "  for v in label:\r\n",
    "    num_label.append(dict_label_to_num[v])\r\n",
    "  return num_label\r\n",
    "\r\n",
    "def num_to_label(label):\r\n",
    "  \"\"\"\r\n",
    "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\r\n",
    "  \"\"\"\r\n",
    "  origin_label = []\r\n",
    "  with open('/opt/ml/code/dict_num_to_label.pkl', 'rb') as f:\r\n",
    "    dict_num_to_label = pickle.load(f)\r\n",
    "  for v in label:\r\n",
    "    origin_label.append(dict_num_to_label[v])\r\n",
    "  \r\n",
    "  return origin_label\r\n",
    "  \r\n",
    "def init_logger():\r\n",
    "    logging.basicConfig(\r\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
    "        level=logging.INFO,\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def compute_metrics(preds, labels):\r\n",
    "    assert len(preds) == len(labels)\r\n",
    "    return acc_and_f1(preds, labels)\r\n",
    "\r\n",
    "\r\n",
    "def simple_accuracy(preds, labels):\r\n",
    "    return (preds == labels).mean()\r\n",
    "\r\n",
    "\r\n",
    "def official_f1():\r\n",
    "\r\n",
    "    with open(os.path.join('/opt/ml/eval/result.txt'), \"r\", encoding=\"utf-8\") as f:\r\n",
    "        macro_result = list(f)[-1]\r\n",
    "        macro_result = macro_result.split(\":\")[1].replace(\">>>\", \"\").strip()\r\n",
    "        macro_result = macro_result.split(\"=\")[1].strip().replace(\"%\", \"\")\r\n",
    "        macro_result = float(macro_result) / 100\r\n",
    "\r\n",
    "    return macro_result\r\n",
    "\r\n",
    "def acc_and_f1(preds, labels, average=\"macro\"):\r\n",
    "    acc = simple_accuracy(preds, labels)\r\n",
    "    return {\r\n",
    "        \"acc\": acc,\r\n",
    "        #\"f1\": official_f1(),\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def convert_sentence_to_features(train_dataset, tokenizer, max_len, mode='train'):\r\n",
    "    max_seq_len=max_len\r\n",
    "    cls_token=tokenizer.cls_token\r\n",
    "    cls_token_segment_id=tokenizer.cls_token_id\r\n",
    "    pad_token=1\r\n",
    "    pad_token_segment_id=0\r\n",
    "    sequence_a_segment_id=0\r\n",
    "    mask_padding_with_zero=True\r\n",
    "    \r\n",
    "    all_input_ids = []\r\n",
    "    all_attention_mask = []\r\n",
    "    all_token_type_ids = []\r\n",
    "    all_e1_mask=[]\r\n",
    "    all_e2_mask=[]\r\n",
    "    m_len=0\r\n",
    "    for idx in tqdm(train_dataset['id']):\r\n",
    "        temp_sent =  train_dataset['sentence'][idx]\r\n",
    "        sentence =  temp_sent[:train_dataset['sub_st'][idx]] \\\r\n",
    "            + ' <e1> ' + temp_sent[train_dataset['sub_st'][idx]:train_dataset['sub_end'][idx]+1] \\\r\n",
    "            + ' </e1> ' + temp_sent[train_dataset['sub_end'][idx]+1:train_dataset['obj_st'][idx]] \\\r\n",
    "            + ' <e2> ' + temp_sent[train_dataset['obj_st'][idx]:train_dataset['obj_end'][idx]+1] \\\r\n",
    "            + ' </e2> ' + temp_sent[train_dataset['obj_end'][idx]+1:]\r\n",
    "        #print(sentence)\r\n",
    "        \r\n",
    "        token = tokenizer.tokenize(sentence)\r\n",
    "        m_len = max(m_len, len(token))\r\n",
    "        e11_p = token.index(\"<e1>\")  # the start position of entity1\r\n",
    "        e12_p = token.index(\"</e1>\")  # the end position of entity1\r\n",
    "        e21_p = token.index(\"<e2>\")  # the start position of entity2\r\n",
    "        e22_p = token.index(\"</e2>\")  # the end position of entity2\r\n",
    "\r\n",
    "        token[e11_p] = \"$\"\r\n",
    "        token[e12_p] = \"$\"\r\n",
    "        token[e21_p] = \"#\"\r\n",
    "        token[e22_p] = \"#\"\r\n",
    "\r\n",
    "        #print(token)\r\n",
    "\r\n",
    "        e11_p += 1\r\n",
    "        e12_p += 1\r\n",
    "        e21_p += 1\r\n",
    "        e22_p += 1\r\n",
    "\r\n",
    "        special_tokens_count = 4\r\n",
    "        if len(token) < max_seq_len - special_tokens_count:\r\n",
    "\r\n",
    "            token_type_ids = [sequence_a_segment_id] * len(token)\r\n",
    "\r\n",
    "            #add cls_token \r\n",
    "            token = [cls_token] + token \r\n",
    "            token_type_ids = [cls_token_segment_id] + token_type_ids\r\n",
    "\r\n",
    "            \r\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(token)\r\n",
    "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\r\n",
    "\r\n",
    "            padding_length = max_seq_len - len(input_ids)\r\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\r\n",
    "            attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\r\n",
    "            token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\r\n",
    "\r\n",
    "            e1_mask = [0] * len(attention_mask)\r\n",
    "            e2_mask = [0] * len(attention_mask)\r\n",
    "\r\n",
    "            for i in range(e11_p, e12_p + 1):\r\n",
    "                e1_mask[i] = 1\r\n",
    "            for i in range(e21_p, e22_p + 1):\r\n",
    "                e2_mask[i] = 1\r\n",
    "\r\n",
    "            assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\r\n",
    "            assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(\r\n",
    "                len(attention_mask), max_seq_len\r\n",
    "            )\r\n",
    "            assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(\r\n",
    "                    len(token_type_ids), max_seq_len)\r\n",
    "\r\n",
    "\r\n",
    "            all_input_ids.append(input_ids)\r\n",
    "            all_attention_mask.append(attention_mask)\r\n",
    "            all_token_type_ids.append(token_type_ids)\r\n",
    "            all_e1_mask.append(e1_mask)\r\n",
    "            all_e2_mask.append(e2_mask)\r\n",
    "\r\n",
    "\r\n",
    "    all_features = {\r\n",
    "        'input_ids' : torch.tensor(all_input_ids),\r\n",
    "        'attention_mask' : torch.tensor(all_attention_mask),\r\n",
    "        'token_type_ids' : torch.tensor(all_token_type_ids),\r\n",
    "        'e1_mask' : torch.tensor(all_e1_mask),\r\n",
    "        'e2_mask' : torch.tensor(all_e2_mask)\r\n",
    "    }  \r\n",
    "    return all_features\r\n",
    "    \r\n",
    "def init_logger():\r\n",
    "    logging.basicConfig(\r\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\r\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\r\n",
    "        level=logging.INFO,\r\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class FCLayer(nn.Module):\r\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\r\n",
    "        super(FCLayer, self).__init__()\r\n",
    "        self.use_activation = use_activation\r\n",
    "        self.dropout = nn.Dropout(dropout_rate)\r\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\r\n",
    "        self.tanh = nn.Tanh()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.dropout(x)\r\n",
    "        if self.use_activation:\r\n",
    "            x = self.tanh(x)\r\n",
    "        return self.linear(x)\r\n",
    "\r\n",
    "\r\n",
    "class Roberta_RE(RobertaModel):\r\n",
    "    def __init__(self,  model_name, config, dropout_rate):\r\n",
    "        super(Roberta_RE, self).__init__(config)\r\n",
    "        self.model = RobertaModel.from_pretrained(model_name, config=config)  # koelectra\r\n",
    "\r\n",
    "        self.num_labels = config.num_labels\r\n",
    "\r\n",
    "        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\r\n",
    "        # self.eos_fc_layer = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\r\n",
    "        self.entity_fc_layer1 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\r\n",
    "        self.entity_fc_layer2 = FCLayer(config.hidden_size, config.hidden_size, dropout_rate)\r\n",
    "\r\n",
    "        self.label_classifier = FCLayer(\r\n",
    "            config.hidden_size * 3,\r\n",
    "            config.num_labels,\r\n",
    "            dropout_rate,\r\n",
    "            use_activation=False,\r\n",
    "        )\r\n",
    "\r\n",
    "    @staticmethod\r\n",
    "    def entity_average(hidden_output, e_mask):\r\n",
    "        \"\"\"\r\n",
    "        Average the entity hidden state vectors (H_i ~ H_j)\r\n",
    "        :param hidden_output: [batch_size, j-i+1, dim]\r\n",
    "        :param e_mask: [batch_size, max_seq_len]\r\n",
    "                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\r\n",
    "        :return: [batch_size, dim]\r\n",
    "        \"\"\"\r\n",
    "        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\r\n",
    "        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\r\n",
    "\r\n",
    "        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\r\n",
    "        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\r\n",
    "        avg_vector = sum_vector.float() / length_tensor.float()  # broadcasting\r\n",
    "        return avg_vector\r\n",
    "\r\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels, e1_mask, e2_mask):\r\n",
    "        outputs = self.model(\r\n",
    "            input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids\r\n",
    "        )  # sequence_output, pooled_output, (hidden_states), (attentions)\r\n",
    "        sequence_output = outputs[0] #batch, max_len, hidden_size 16, 280, 768\r\n",
    "        pooled_output = outputs[1]  # [CLS] \r\n",
    "\r\n",
    "        # cls_mask = input_ids.eq(0) # cls\r\n",
    "\r\n",
    "        # sentence_representation_cls = sequence_output[cls_mask, :].view(sequence_output.size(0), -1, sequence_output.size(-1))[:,-1,:]\r\n",
    "    \r\n",
    "\r\n",
    "        # eos_mask = input_ids.eq(1) # eos token id = 1\r\n",
    "        \r\n",
    "        # sentence_representation = sequence_output[eos_mask, :].view(sequence_output.size(0), -1, sequence_output.size(-1))[:,-1,:]\r\n",
    "    \r\n",
    "        e1_h = self.entity_average(sequence_output, e1_mask)\r\n",
    "        e2_h = self.entity_average(sequence_output, e2_mask)\r\n",
    "        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\r\n",
    "        # sentence_representation_cls = self.cls_fc_layer(sentence_representation_cls)\r\n",
    "        pooled_output = self.cls_fc_layer(pooled_output)\r\n",
    "        e1_h = self.entity_fc_layer1(e1_h)\r\n",
    "        e2_h = self.entity_fc_layer2(e2_h)\r\n",
    "        # Concat -> fc_layer\r\n",
    "        #concat_h = torch.cat([pooled_output, e1_h, e2_h, torch.abs(torch.sub(e1_h,e2_h))], dim=-1)\r\n",
    "        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=-1)\r\n",
    "        logits = self.label_classifier(concat_h)\r\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n",
    "        # Softmax\r\n",
    "        if labels is not None:\r\n",
    "            if self.num_labels == 1:\r\n",
    "                loss_fct = nn.MSELoss()\r\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\r\n",
    "            else:\r\n",
    "                loss_fct = nn.CrossEntropyLoss()\r\n",
    "                #loss_fct = nn.BCEWithLogitsLoss()\r\n",
    "                #loss_fct = LabelSmoothingCrossEntropy()\r\n",
    "                #loss_fct = Cross_FocalLoss()\r\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n",
    "\r\n",
    "            outputs = (loss,) + outputs\r\n",
    "\r\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logger = logging.getLogger(__name__)\r\n",
    "class Trainer(object):\r\n",
    "    def __init__(self,num_labels,logging_steps, save_steps,max_steps,\r\n",
    "                 num_train_epochs,warmup_steps,adam_epsilon,learning_rate,gradient_accumulation_steps,\r\n",
    "                 max_grad_norm, eval_batch_size, train_batch_size, model_dir, dropout_rate, classifier_epoch, tokenizer,\r\n",
    "                 weight_decay,train_dataset=None, dev_dataset=None, test_dataset=None):\r\n",
    "        #self.args = args\r\n",
    "        self.train_dataset = train_dataset\r\n",
    "        self.eval_batch_size = eval_batch_size\r\n",
    "        self.train_batch_size = train_batch_size\r\n",
    "        self.dev_dataset = dev_dataset\r\n",
    "        self.test_dataset = test_dataset\r\n",
    "        #self.Model_name = Model_name\r\n",
    "        #self.label_lst = label_dict\r\n",
    "        self.num_labels = num_labels\r\n",
    "        self.max_steps = max_steps\r\n",
    "        self.weight_decay = weight_decay\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.adam_epsilon=adam_epsilon\r\n",
    "        self.warmup_steps = warmup_steps\r\n",
    "        self.num_train_epochs = num_train_epochs\r\n",
    "        self.logging_steps = logging_steps\r\n",
    "        self.save_steps = save_steps\r\n",
    "        self.max_grad_norm = max_grad_norm\r\n",
    "        self.model_dir = model_dir\r\n",
    "        self.dropout_rate = dropout_rate\r\n",
    "        self.classifier_epoch=classifier_epoch\r\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\r\n",
    "        self.global_epo=0\r\n",
    "        self.config = AutoConfig.from_pretrained(\r\n",
    "            'klue/roberta-large',\r\n",
    "            num_labels=self.num_labels\r\n",
    "        )\r\n",
    "        self.model = Roberta_RE(\r\n",
    "            'klue/roberta-large', config=self.config, dropout_rate = self.dropout_rate,\r\n",
    "        )\r\n",
    "        self.model.config.vocab_size = len(tokenizer)\r\n",
    "\r\n",
    "        # GPU or CPU\r\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\r\n",
    "        self.model.to(self.device)\r\n",
    "        \r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        train_sampler = RandomSampler(self.train_dataset)\r\n",
    "        train_dataloader = DataLoader(\r\n",
    "            self.train_dataset,\r\n",
    "            sampler=train_sampler,\r\n",
    "            batch_size=self.train_batch_size,\r\n",
    "        )\r\n",
    "\r\n",
    "        if self.max_steps > 0:\r\n",
    "            t_total = self.max_steps\r\n",
    "            self.num_train_epochs = (\r\n",
    "                self.max_steps // (len(train_dataloader) // self.gradient_accumulation_steps) + 1\r\n",
    "            )\r\n",
    "        else:\r\n",
    "            t_total = len(train_dataloader) // self.gradient_accumulation_steps * self.num_train_epochs\r\n",
    "\r\n",
    "        # Prepare optimizer and schedule (linear warmup and decay)\r\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\r\n",
    "        optimizer_grouped_parameters = [\r\n",
    "            {\r\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\r\n",
    "                \"weight_decay\": self.weight_decay,\r\n",
    "            },\r\n",
    "            {\r\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\r\n",
    "                \"weight_decay\": 0.0,\r\n",
    "            },\r\n",
    "        ]\r\n",
    "        optimizer = AdamW(\r\n",
    "            optimizer_grouped_parameters,\r\n",
    "            lr=self.learning_rate,\r\n",
    "            eps=self.adam_epsilon,\r\n",
    "        )\r\n",
    "        scheduler = get_linear_schedule_with_warmup(\r\n",
    "            optimizer,\r\n",
    "            num_warmup_steps=self.warmup_steps,\r\n",
    "            num_training_steps=t_total,\r\n",
    "        )\r\n",
    "        \r\n",
    "        #scaler = torch.cuda.amp.GradScaler()\r\n",
    "        # Train!\r\n",
    "        logger.info(\"***** Running training *****\")\r\n",
    "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\r\n",
    "        logger.info(\"  Num Epochs = %d\", self.num_train_epochs)\r\n",
    "        logger.info(\"  Total train batch size = %d\", self.train_batch_size)\r\n",
    "        logger.info(\"  Gradient Accumulation steps = %d\", self.gradient_accumulation_steps)\r\n",
    "        logger.info(\"  Total optimization steps = %d\", t_total)\r\n",
    "        logger.info(\"  Logging steps = %d\", self.logging_steps)\r\n",
    "        logger.info(\"  Save steps = %d\", self.save_steps)\r\n",
    "\r\n",
    "        global_step = 0\r\n",
    "        tr_loss = 0.0\r\n",
    "        self.model.zero_grad()\r\n",
    "\r\n",
    "        train_iterator = tqdm(range(int(self.num_train_epochs)), desc=\"Epoch\")\r\n",
    "\r\n",
    "        for epo_step in train_iterator:\r\n",
    "            self.global_epo = epo_step\r\n",
    "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\r\n",
    "            for step, batch in enumerate(epoch_iterator):\r\n",
    "                self.model.train()\r\n",
    "                batch = tuple(batch[t].to(self.device) for t in batch)  # GPU or CPU\r\n",
    "                inputs = {\r\n",
    "                    \"input_ids\": batch[0],\r\n",
    "                    \"attention_mask\": batch[1],\r\n",
    "                    \"token_type_ids\" : batch[2],\r\n",
    "                    \"labels\": batch[5],\r\n",
    "                    \"e1_mask\": batch[3],\r\n",
    "                    \"e2_mask\": batch[4]\r\n",
    "                }\r\n",
    "                #with torch.cuda.amp.autocast():\r\n",
    "                outputs = self.model(**inputs)\r\n",
    "                loss = outputs[0]\r\n",
    "\r\n",
    "                if self.gradient_accumulation_steps > 1:\r\n",
    "                    loss = loss / self.gradient_accumulation_steps\r\n",
    "\r\n",
    "                #scaler.scale(loss).backward()\r\n",
    "                loss.backward()\r\n",
    "\r\n",
    "                tr_loss += loss.item()\r\n",
    "                if (step + 1) % self.gradient_accumulation_steps == 0:\r\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\r\n",
    "\r\n",
    "                    optimizer.step()\r\n",
    "                    #scaler.step(optimizer)\r\n",
    "                    #scaler.update()\r\n",
    "                    scheduler.step()  # Update learning rate schedule\r\n",
    "                    self.model.zero_grad()\r\n",
    "                    global_step += 1\r\n",
    "\r\n",
    "                    if self.logging_steps > 0 and global_step % self.logging_steps == 0:\r\n",
    "                        logger.info(\"  global steps = %d\", global_step)\r\n",
    "                        self.evaluate(\"dev\")\r\n",
    "                    if self.save_steps > 0 and global_step % self.save_steps == 0:\r\n",
    "                        self.save_model()\r\n",
    "\r\n",
    "                if 0 < self.max_steps < global_step:\r\n",
    "                    epoch_iterator.close()\r\n",
    "                    break\r\n",
    "\r\n",
    "            \r\n",
    "\r\n",
    "            if 0 < self.max_steps < global_step:\r\n",
    "                train_iterator.close()\r\n",
    "                break\r\n",
    "          \r\n",
    "\r\n",
    "        return global_step, tr_loss / global_step\r\n",
    "    \r\n",
    "   \r\n",
    "    def evaluate(self, mode):\r\n",
    "        # We use test dataset because semeval doesn't have dev dataset\r\n",
    "        if mode == \"test\":\r\n",
    "            dataset = self.test_dataset\r\n",
    "        elif mode == \"dev\":\r\n",
    "            dataset = self.dev_dataset\r\n",
    "        elif mode == \"train\":\r\n",
    "            dataset = self.train_dataset\r\n",
    "        else:\r\n",
    "            raise Exception(\"Only dev and test dataset available\")\r\n",
    "\r\n",
    "        eval_sampler = SequentialSampler(dataset)\r\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.eval_batch_size)\r\n",
    "\r\n",
    "        # Eval!\r\n",
    "        logger.info('---------------------------------------------------')\r\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\r\n",
    "        logger.info(\"  Num examples = %d\", len(dataset))\r\n",
    "        logger.info(\"  Batch size = %d\", self.eval_batch_size)\r\n",
    "        eval_loss = 0.0\r\n",
    "        nb_eval_steps = 0\r\n",
    "        preds = None\r\n",
    "        out_label_ids = None\r\n",
    "\r\n",
    "        self.model.eval()\r\n",
    "\r\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\r\n",
    "            batch = tuple(batch[t].to(self.device) for t in batch)\r\n",
    "            with torch.no_grad():\r\n",
    "                inputs = {\r\n",
    "                    \"input_ids\": batch[0],\r\n",
    "                    \"attention_mask\": batch[1],\r\n",
    "                    \"token_type_ids\": batch[2],\r\n",
    "                    \"labels\": batch[5],\r\n",
    "                    \"e1_mask\": batch[3],\r\n",
    "                    \"e2_mask\": batch[4],\r\n",
    "                }\r\n",
    "                #with torch.cuda.amp.autocast():\r\n",
    "                outputs = self.model(**inputs)\r\n",
    "                tmp_eval_loss, logits = outputs[:2]\r\n",
    "                eval_loss += tmp_eval_loss.mean().item()\r\n",
    "            nb_eval_steps += 1\r\n",
    "\r\n",
    "            if preds is None:\r\n",
    "                preds = logits.detach().cpu().numpy()\r\n",
    "                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\r\n",
    "            else:\r\n",
    "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\r\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\r\n",
    "\r\n",
    "        eval_loss = eval_loss / nb_eval_steps\r\n",
    "        results = {\"loss\": eval_loss}\r\n",
    "        preds = np.argmax(preds, axis=1)\r\n",
    "        # preds = np.around(preds)\r\n",
    "        # preds = preds.astype(int)\r\n",
    "        result = compute_metrics(preds,out_label_ids)\r\n",
    "        results.update(result)\r\n",
    "\r\n",
    "        logger.info(\"***** Eval results *****\")\r\n",
    "        for key in sorted(results.keys()):\r\n",
    "            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\r\n",
    "            # if key == 'acc':\r\n",
    "            #     if results[key] > 0.85:\r\n",
    "            #         self.test_pred()\r\n",
    "        logger.info(\"---------------------------------------------------\")\r\n",
    "        return results\r\n",
    "    \r\n",
    "    def test_pred(self):\r\n",
    "        test_dataset = self.test_dataset\r\n",
    "        test_sampler = SequentialSampler(test_dataset)\r\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler,batch_size=self.eval_batch_size)\r\n",
    "\r\n",
    "        # Eval!\r\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", \"test\")\r\n",
    "        #logger.info(\"  Num examples = %d\", len(dataset))\r\n",
    "        logger.info(\"  Batch size = %d\", self.eval_batch_size)\r\n",
    "\r\n",
    "        nb_eval_steps = 0\r\n",
    "        preds = None\r\n",
    "        probs = None\r\n",
    "        out_label_ids = None\r\n",
    "\r\n",
    "        self.model.eval()\r\n",
    "\r\n",
    "        for batch in tqdm(test_dataloader, desc=\"Predicting\"):\r\n",
    "            batch = tuple(batch[t].to(self.device) for t in batch)\r\n",
    "            with torch.no_grad():\r\n",
    "                inputs = {\r\n",
    "                    \"input_ids\": batch[0],\r\n",
    "                    \"attention_mask\": batch[1],\r\n",
    "                    \"token_type_ids\": batch[2],\r\n",
    "                    \"labels\": None,\r\n",
    "                    \"e1_mask\": batch[3],\r\n",
    "                    \"e2_mask\": batch[4],\r\n",
    "                }\r\n",
    "                outputs = self.model(**inputs)\r\n",
    "                #print(outputs)\r\n",
    "                logits = outputs[0]\r\n",
    "\r\n",
    "            nb_eval_steps += 1\r\n",
    "            prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\r\n",
    "\r\n",
    "            if preds is None:\r\n",
    "                probs = prob\r\n",
    "                preds = prob.argmax(logits, axis=-1)\r\n",
    "                #out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\r\n",
    "            else:\r\n",
    "                probs = np.append(probs, prob, axis=0)\r\n",
    "                preds = np.append(preds, prob.argmax(logits, axis=-1), axis=0)\r\n",
    "                #out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\r\n",
    "\r\n",
    "        # df = pd.DataFrame(preds, columns=['pred'])\r\n",
    "        # df.to_csv('RXLMRoberta_layersplit_with_focalcross_epoch'+str(self.global_epo)+'.csv', index=False)\r\n",
    "#         with open(\"proposed_answers.txt\", \"w\", encoding=\"utf-8\") as f:\r\n",
    "#             for idx, pred in enumerate(preds):\r\n",
    "#                 f.write(\"{}\\n\".format(pred))\r\n",
    "        #write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers.txt\"), preds)\r\n",
    "        return np.concatenate(preds).tolist(), np.concatenate(probs, axis=0).tolist()\r\n",
    "    \r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        # Save model checkpoint (Overwrite)\r\n",
    "        if not os.path.exists(self.model_dir):\r\n",
    "            os.makedirs(self.model_dir)\r\n",
    "        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\r\n",
    "        model_to_save.save_pretrained(self.model_dir)\r\n",
    "\r\n",
    "        # Save training arguments together with the trained model\r\n",
    "        #torch.save(self.args, os.path.join(self.args.model_dir, \"training_args.bin\"))\r\n",
    "        logger.info(\"Saving model checkpoint to %s\", self.model_dir)\r\n",
    "\r\n",
    "    def load_model(self):\r\n",
    "        # Check whether model exists\r\n",
    "        if not os.path.exists(self.model_dir):\r\n",
    "            raise Exception(\"Model doesn't exists! Train first!\")\r\n",
    "\r\n",
    "        #self.args = torch.load(os.path.join(self.args.model_dir, \"training_args.bin\"))\r\n",
    "        self.model = RobertaModel.from_pretrained(\"klue/roberta-large\")\r\n",
    "        self.model.to(self.device)\r\n",
    "        logger.info(\"***** Model Loaded *****\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "init_logger()\r\n",
    "\r\n",
    "train_dir = '/opt/ml/dataset/train/train.csv'\r\n",
    "test_dir = '/opt/ml/dataset/test/test_data.csv'\r\n",
    "\r\n",
    "train_dataset = load_data(train_dir)\r\n",
    "\r\n",
    "train_dataset, dev_dataset = train_test_split(train_dataset, test_size=0.1 ,stratify=train_dataset['label'],\\\r\n",
    "                                             shuffle=True, random_state= random_seed)\r\n",
    "train_label  = label_to_num(train_dataset['label'].values)\r\n",
    "dev_label = label_to_num(dev_dataset['label'].values)\r\n",
    "\r\n",
    "\r\n",
    "test_dataset = load_data(test_dir)\r\n",
    "test_label = list(map(int,test_dataset['label'].values))\r\n",
    "\r\n",
    "\r\n",
    "ADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"</e1>\", \"<e2>\", \"</e2>\"]\r\n",
    "\r\n",
    "tokenizer =  AutoTokenizer.from_pretrained(\"klue/roberta-large\")\r\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\r\n",
    "\r\n",
    "train_feature = convert_sentence_to_features(train_dataset, tokenizer, max_len = 441 , mode='train')\r\n",
    "dev_feature = convert_sentence_to_features(dev_dataset, tokenizer, max_len = 441 , mode='train')\r\n",
    "test_feature = convert_sentence_to_features(test_dataset, tokenizer, max_len = 441 , mode='train')\r\n",
    "\r\n",
    "Train_dataset = RE_Dataset(train_feature,train_label)\r\n",
    "Dev_dataset = RE_Dataset(dev_feature,dev_label)\r\n",
    "Test_dataset = RE_Dataset(test_feature,test_label)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer = Trainer(eval_batch_size=8,train_batch_size=8, num_labels = 30,\r\n",
    "                  max_steps=-1, weight_decay=0.001, learning_rate= 5e-5, \r\n",
    "                  adam_epsilon=1e-8, warmup_steps=20, num_train_epochs=10,\r\n",
    "                  logging_steps= 1600, save_steps= 800, max_grad_norm=1.0, \r\n",
    "                  model_dir='./model2', gradient_accumulation_steps=4,dropout_rate = 0.1, classifier_epoch=3, tokenizer = tokenizer,\r\n",
    "                  train_dataset=Train_dataset,\r\n",
    "                  dev_dataset=Dev_dataset,\r\n",
    "                  test_dataset = Test_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "trainer.train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "config = AutoConfig.from_pretrained(\"klue/roberta-large\", num_labels=30)\r\n",
    "path = \"./model/pytorch_model.bin\"\r\n",
    "model = Roberta_RE('klue/roberta-large', config=config, dropout_rate=0.1)\r\n",
    "model.load_state_dict(torch.load(path))\r\n",
    "device = torch.device('cuda:0')\r\n",
    "loader = DataLoader(Test_dataset, batch_size=64, shuffle=False)\r\n",
    "model.eval()\r\n",
    "model.to(device)\r\n",
    "output_pred = []\r\n",
    "output_prob = []\r\n",
    "for idx, data in enumerate(tqdm(loader)):\r\n",
    "    with torch.no_grad():\r\n",
    "        outputs = model(\r\n",
    "            input_ids=data['input_ids'].to(device),\r\n",
    "            attention_mask=data['attention_mask'].to(device),\r\n",
    "            token_type_ids=data['token_type_ids'].to(device),\r\n",
    "            e1_mask = data['e1_mask'].to(device),\r\n",
    "            e2_mask = data['e2_mask'].to(device),\r\n",
    "            labels=None,\r\n",
    "        )\r\n",
    "        logits = outputs[0]\r\n",
    "        prob = F.softmax(logits, dim=-1).cpu().numpy()\r\n",
    "        logits = logits.cpu().numpy()\r\n",
    "        result = np.argmax(logits, axis=-1)\r\n",
    "\r\n",
    "        output_pred.append(result)\r\n",
    "        output_prob.append(prob)\r\n",
    "\r\n",
    "\r\n",
    "output_pred = np.concatenate(output_pred)\r\n",
    "output_prob = np.concatenate(output_prob)\r\n",
    "submission = pd.DataFrame({\r\n",
    "    \"id\":range(len(output_prob)),\r\n",
    "    \"pred_label\":num_to_label(output_pred),\r\n",
    "    \"probs\":list(map(lambda x:str(list(x)) , output_prob)),\r\n",
    "    })\r\n",
    "submission.to_csv(\"R-Roberta.csv\",index=False)"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}